{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDM27gQHDWBl"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "import textwrap\n",
    "from typing import Literal\n",
    "\n",
    "from datasets import Dataset\n",
    "import optuna\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "  AutoModelForCausalLM, AutoTokenizer,\n",
    "  BitsAndBytesConfig,\n",
    "  DataCollatorForLanguageModeling,\n",
    "  EarlyStoppingCallback,\n",
    "  TextGenerationPipeline,\n",
    "  Trainer, TrainerCallback, TrainerControl,\n",
    "  TrainerState, TrainingArguments,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QL2UEUypcy2h"
   },
   "source": [
    "# Demonstration of Granite-3B-Code-Base-2K model finetuning for C++ code completion\n",
    "\n",
    "## Introduction\n",
    "In this notebook is demonstrated how a [Granite-3B-Code-Base-2K](#Granite-3B-Code-Base-2K) model can be finetuned for C++ code completion, based on *.cc* and *.h* files in a folder. The goal is to limit the required computing resources through [quantization](https://huggingface.co/docs/transformers/quantization/overview) and [parameter-efficient fine-tuning (PEFT)](https://huggingface.co/docs/peft/index). The demonstrated method utilizes the [Hugging Face Transformers](#Hugging-Face-Transformers) and [PyTorch](#PyTorch) deep learning libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVevv34wMVMV"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YO0_RJZgBYzn"
   },
   "outputs": [],
   "source": [
    "# General\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Data\n",
    "DATA_DIR = \"data\"\n",
    "SAMPLE_SPAN = 3\n",
    "SAMPLE_STRIDE = 1\n",
    "\n",
    "# Optuna\n",
    "OPTUNA_ENABLED = False\n",
    "OPTUNA_EPOCHS = 2\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 32\n",
    "EARLY_STOP_PATIENCE = 5\n",
    "LEARN_RATE = 1e-4\n",
    "LEARN_RATE_SCHEDULER = \"inverse_sqrt\"\n",
    "MAX_TRAIN_EPOCHS = 1_000_000\n",
    "MODEL_NAME = \"ibm-granite/granite-3b-code-base-2k\"\n",
    "MODELS_DIR = \"models\"\n",
    "TRAIN_EPOCHS = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "WARMUP_STEPS = 0\n",
    "WEIGHT_DECAY = 0.05\n",
    "\n",
    "# Text generation\n",
    "MAX_GEN_TOKENS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmLe5_F5gzvl"
   },
   "source": [
    "## Dataset retrieval\n",
    "\n",
    "The dataset constitutes the lines of all C++ source and header files which can be found at `DATA_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSN56LKWgzvl"
   },
   "outputs": [],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "cpp_path = Path(DATA_DIR)\n",
    "cpp_filepaths = list(cpp_path.glob(\"**/*.cc\")) + list(cpp_path.glob(\"**/*.h\"))\n",
    "raw_lines = []\n",
    "for filepath in cpp_filepaths:\n",
    "  raw_lines += filepath.read_text().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srqKC86FH1vH"
   },
   "source": [
    "## Tokenizer\n",
    "\n",
    "The [Granite-3B-Code-Base-2K](#Granite-3B-Code-Base-2K) model uses [Byte pair encoding](#Byte-pair-encoding) for its inputs. Thus, spaces, new lines and braces, which are important not just for C++ but also for many other programming languages, are part of the vocabulary and while their embeddings are \"freezed\", the model can still learn new and specific usages as per the codebase at `DATA_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Ky-3zAlH1vI"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "inverted_vocab = {v: k for k, v in tokenizer.get_vocab().items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_muHj8_kl1zC"
   },
   "source": [
    "## Dataset preprocessing\n",
    "\n",
    "First, single-line comments, *#include*s and empty lines are removed.\n",
    "\n",
    "Then, based on the `SAMPLE_SPAN` and `SAMPLE_STRIDE` settings, the dataset lines are combined into potentially overlapping groups of length `SAMPLE_SPAN` and offset `SAMPLE_STRIDE`.\n",
    "\n",
    "The formed groups are then shuffled and split into training and validation sets, based on the `VALIDATION_SPLIT` setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MI42TOiCl5l4"
   },
   "outputs": [],
   "source": [
    "raw_lines = [re.sub(r\"([^\\s]*)\\s*//.*\", r\"\\1\", line)\n",
    "             for line in raw_lines if line]\n",
    "raw_lines = [line\n",
    "             for line in raw_lines\n",
    "             if line and not line.lstrip().startswith(\"#include\")]\n",
    "\n",
    "samples = [\"\\n\".join(raw_lines[i:i+SAMPLE_SPAN])\n",
    "           for i in range(0, len(raw_lines)-SAMPLE_SPAN+1, SAMPLE_STRIDE)]\n",
    "random.shuffle(samples)\n",
    "\n",
    "val_idx = int(VALIDATION_SPLIT * len(samples))\n",
    "train_samples = samples[:-val_idx]\n",
    "train_ds = Dataset.from_dict(tokenizer(train_samples))\n",
    "val_samples = samples[-val_idx:]\n",
    "val_ds = Dataset.from_dict(tokenizer(val_samples))\n",
    "train_ds, val_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1YhnVSIr5fz"
   },
   "source": [
    "## Preparation for training\n",
    "\n",
    "In order to limit the system resources required for training, on the model is performed 8-bit [quantization](https://huggingface.co/docs/transformers/quantization/overview).\n",
    "In order to preserve the pretrained model weights, `PEFT` is performed, based on the model architecture.\n",
    "Standard [Hugging Face Transformers](#Hugging-Face-Transformers) API for [PyTorch](#PyTorch), such as the `Trainer`, `TrainingArguments` and `TrainerCallback` classes, is utilized to configure the [Granite-3B-Code-Base-2K](#Granite-3B-Code-Base-2K) model for training.\n",
    "\n",
    "If the `OPTUNA_ENABLED` setting is *True*, then, instead of a training process, starts a hyperparameter tuning process via [Optuna](#Optuna). Thus, settings such as learning rate value and schedule, batch size and weight decay can be tuned.\n",
    "\n",
    "Additionally, a custom `CustomCallback` (based on `TrainerCallback`) is implemented to provide training control and, via the *evaluate_model* function, evaluation of the model's text generation capabilities on:\n",
    "- a single, randomly selected, C++ code sample per training epoch;\n",
    "- a custom C++ fragment during inference.\n",
    "\n",
    "The *evaluate_model* function displays various evaluation data such as:\n",
    "- the C++ code fragment;\n",
    "- the tokens' representation as per the tokenizer;\n",
    "- the prompt as per the *prompt_strategy* parameter, i.e., whether the first half of *cpp_text*, the second half or the whole *cpp_text* is to be used as prompt for code completion;\n",
    "- the generated code without the prompt, as per the *suggest_changes* function.\n",
    "\n",
    "The generated code is displayed only up to `MAX_GEN_TOKENS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FgeGzmUSr8BS"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "  model: AutoModelForCausalLM,\n",
    "  tokenizer: AutoTokenizer,\n",
    "  dataset: Dataset | None = None,\n",
    "  cpp_text: str | None = None,\n",
    "  prompt_strategy: Literal[\"start\", \"end\", \"all\"] = \"all\",\n",
    ") -> None:\n",
    "  if dataset is None and cpp_text is None:\n",
    "    raise ValueError(\"evaluate_model: \"\n",
    "                     \"one of dataset and cpp_text must be set.\")\n",
    "  model.eval()  # type: ignore\n",
    "  if dataset is not None:\n",
    "    index = random.randint(0, len(dataset)-1)\n",
    "    ids = dataset[\"input_ids\"][index]\n",
    "    cpp_text = tokenizer.decode(ids)  # type: ignore\n",
    "  else:\n",
    "    ids = tokenizer.encode(cpp_text)  # type: ignore\n",
    "  print(\"-\" * 80,\n",
    "        f\"[ C++ CODE ]\\n\\n{cpp_text}\",\n",
    "        sep=\"\\n\")\n",
    "\n",
    "  tokens_repr = \" \".join([inverted_vocab[i] for i in ids])\n",
    "  tokens_lines = textwrap.wrap(tokens_repr, width=80,\n",
    "                               expand_tabs=False,\n",
    "                               replace_whitespace=False,\n",
    "                               break_long_words=False,\n",
    "                               break_on_hyphens=False,\n",
    "                               drop_whitespace=False)\n",
    "  print(\"-\" * 80)\n",
    "  print(\"[ TOKENS ]\\n\\n\")\n",
    "  for line in tokens_lines:\n",
    "    print(line)\n",
    "\n",
    "  if prompt_strategy == \"start\":\n",
    "    prompt_ids = ids[:len(ids)//2]\n",
    "  elif prompt_strategy == \"end\":\n",
    "    prompt_ids = ids[len(ids)//2:]\n",
    "  else:\n",
    "    prompt_ids = ids\n",
    "  prompt = tokenizer.decode(prompt_ids)  # type: ignore\n",
    "  print(\"-\" * 80,\n",
    "        f\"[ PROMPT ]\\n\\n{prompt}\",\n",
    "        sep=\"\\n\")\n",
    "\n",
    "  changes = suggest_changes(model, tokenizer, prompt=prompt)\n",
    "  print(\"-\" * 80,\n",
    "        f\"[ GENERATED ]\\n\\n{changes}\",\n",
    "        \"-\" * 80,\n",
    "        sep=\"\\n\")\n",
    "\n",
    "\n",
    "def suggest_changes(\n",
    "  model: AutoModelForCausalLM,\n",
    "  tokenizer: AutoTokenizer,\n",
    "  prompt: str,\n",
    ") -> str:\n",
    "  if not prompt:\n",
    "    raise ValueError(\"suggest_changes: prompt is empty.\")\n",
    "  pipe = TextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
    "  return pipe(prompt,\n",
    "              max_new_tokens=MAX_GEN_TOKENS,\n",
    "              return_full_text=False)[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=MODELS_DIR,\n",
    "  eval_strategy=\"epoch\",\n",
    "  per_device_train_batch_size=BATCH_SIZE,\n",
    "  per_device_eval_batch_size=BATCH_SIZE,\n",
    "  learning_rate=LEARN_RATE,\n",
    "  lr_scheduler_type=LEARN_RATE_SCHEDULER,\n",
    "  warmup_steps=WARMUP_STEPS,\n",
    "  metric_for_best_model=\"eval_loss\",\n",
    "  save_strategy=\"no\" if OPTUNA_ENABLED else \"epoch\",\n",
    "  save_total_limit=1,\n",
    "  save_only_model=False,\n",
    "  seed=RANDOM_SEED,\n",
    "  dataloader_drop_last=True,\n",
    "  load_best_model_at_end=not OPTUNA_ENABLED,\n",
    "  weight_decay=WEIGHT_DECAY,\n",
    "  report_to=[\"none\"] if OPTUNA_ENABLED else [\"tensorboard\"],\n",
    "  push_to_hub=False,\n",
    "  num_train_epochs=OPTUNA_EPOCHS if OPTUNA_ENABLED else MAX_TRAIN_EPOCHS,\n",
    ")\n",
    "\n",
    "\n",
    "if not OPTUNA_ENABLED:\n",
    "  class CustomCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "      self._remaining_train_epochs = TRAIN_EPOCHS\n",
    "\n",
    "    def on_epoch_end(\n",
    "      self,\n",
    "      args: TrainingArguments,\n",
    "      state: TrainerState,\n",
    "      control: TrainerControl,\n",
    "      model: AutoModelForCausalLM,\n",
    "      processing_class: AutoTokenizer,\n",
    "      train_dataloader: DataLoader | None = None,\n",
    "      **kwargs,\n",
    "    ) -> None:\n",
    "      if train_dataloader is not None:\n",
    "        evaluate_model(model, processing_class,\n",
    "                       dataset=train_dataloader.dataset)  # type: ignore\n",
    "      self._remaining_train_epochs -= 1\n",
    "      if self._remaining_train_epochs == 0:\n",
    "        control.should_training_stop = True\n",
    "\n",
    "\n",
    "def model_init(trial: optuna.Trial) -> AutoModelForCausalLM:\n",
    "  quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "  model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    "  )\n",
    "  lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "  )\n",
    "  model = get_peft_model(model, lora_config)\n",
    "  trainable_params = sum(p.numel()\n",
    "                         for p in model.parameters()\n",
    "                         if p.requires_grad)\n",
    "  print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "  return model  # type: ignore\n",
    "\n",
    "\n",
    "trainer_cbs = None\n",
    "if not OPTUNA_ENABLED:\n",
    "  trainer_cbs = [\n",
    "    CustomCallback(),  # type: ignore\n",
    "    EarlyStoppingCallback(early_stopping_patience=EARLY_STOP_PATIENCE),\n",
    "  ]\n",
    "trainer = Trainer(\n",
    "  model_init=model_init,  # type: ignore\n",
    "  processing_class=tokenizer,\n",
    "  args=training_args,\n",
    "  train_dataset=train_ds,\n",
    "  eval_dataset=val_ds,\n",
    "  data_collator=DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    "  ),\n",
    "  callbacks=trainer_cbs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBPbyMVLNy-r"
   },
   "source": [
    "## Training and evaluation\n",
    "\n",
    "If `OPTUNA_ENABLED` is *True*, then in the `optuna_hp_space` function are configured the trial parameters. The optimization process is performed with regard to the validation loss.\n",
    "\n",
    "If `OPTUNA_ENABLED` is *False*, then a training process starts. The final validation loss and perplexity metrics are reported once the training process completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KhH9LK6HmamW"
   },
   "outputs": [],
   "source": [
    "if OPTUNA_ENABLED:\n",
    "  def optuna_hp_space(trial: optuna.Trial) -> dict[str, str | float]:\n",
    "    return {\n",
    "      \"learning_rate\": trial.suggest_float(\n",
    "        \"learning_rate\", 1e-6, 1e-4, step=1e-6\n",
    "      ),\n",
    "      \"weight_decay\": trial.suggest_float(\n",
    "        \"weight_decay\", 0, 1e-1, step=1e-2\n",
    "      ),\n",
    "      \"lr_scheduler_type\": trial.suggest_categorical(\n",
    "        \"lr_scheduler_type\", [\"constant\", \"cosine\",\n",
    "                              \"inverse_sqrt\", \"linear\",\n",
    "                              \"reduce_lr_on_plateau\"]\n",
    "      ),\n",
    "      \"per_device_train_batch_size\": trial.suggest_categorical(\n",
    "        \"per_device_train_batch_size\", [32]\n",
    "      ),\n",
    "    }\n",
    "  optuna_sampler = optuna.samplers.TPESampler(seed=RANDOM_SEED)\n",
    "  best_trials = trainer.hyperparameter_search(\n",
    "    hp_space=optuna_hp_space,  # type: ignore\n",
    "    direction=\"minimize\",\n",
    "    backend=\"optuna\",\n",
    "    n_trials=20,\n",
    "    sampler=optuna_sampler,\n",
    "  )\n",
    "  print(best_trials)\n",
    "else:\n",
    "  last_checkpoint = get_last_checkpoint(MODELS_DIR)\n",
    "  trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "  eval_results = trainer.evaluate()\n",
    "  print(eval_results)\n",
    "  print(\"Perplexity: \"\n",
    "        f\"{torch.exp(torch.tensor(eval_results['eval_loss'])).item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdtyd5J4bUXD"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFONsS-tUR95"
   },
   "outputs": [],
   "source": [
    "if not OPTUNA_ENABLED:\n",
    "  evaluate_model(trainer.model, tokenizer,  # type: ignore\n",
    "                 cpp_text=samples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xA-LMCCWgzvp"
   },
   "source": [
    "## References\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### APA style for references\n",
    "American Psychological Association. (2022). Creating an APA Style reference list guide. https://apastyle.apa.org/instructional-aids/creating-reference-list.pdf\n",
    "\n",
    "American Psychological Association. (2024). APA Style common reference examples guide. https://apastyle.apa.org/instructional-aids/reference-examples.pdf\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Tokenization\n",
    "#### Byte-pair encoding\n",
    "Sennrich, R., Haddow, B., & Birch, A. (2015). Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909. https://arxiv.org/abs/1508.07909\n",
    "- [Byte pair encoding - Wikipedia](https://en.wikipedia.org/wiki/Byte_pair_encoding)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Machine learning models\n",
    "#### Granite-3B-Code-Base-2K\n",
    "Mishra, M., Stallone, M., Zhang, G., Shen, Y., Prasad, A., Soria, A.M., Merler, M., Selvam, P., Surendran, S., Singh, S., Sethi, M., Dang, X., Li, P., Wu, K., Zawad, S., Coleman, A., White, M., Lewis, M., Pavuluri, R., Koyfman, Y., Lublinsky, B., Bayser, M.D., Abdelaziz, I., Basu, K., Agarwal, M., Zhou, Y., Johnson, C., Goyal, A., Patel, H., Shah, Y., Zerfos, P., Ludwig, H., Munawar, A., Crouse, M., Kapanipathi, P., Salaria, S., Calio, B., Wen, S., Seelam, S.R., Belgodere, B.M., Fonseca, C., Singhee, A., Desai, N., Cox, D.D., Puri, R., & Panda, R. (2024). Granite Code Models: A Family of Open Foundation Models for Code Intelligence. ArXiv, abs/2405.04324. https://arxiv.org/abs/2405.04324\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Guides and tutorials\n",
    "- [ibm-granite/granite-3b-code-base-2k · Hugging Face](https://huggingface.co/ibm-granite/granite-3b-code-base-2k)\n",
    "- [Hugging Face - Documentation](https://huggingface.co/docs)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Libraries\n",
    "#### Hugging Face Transformers\n",
    "Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M., Lhoest, Q., & Rush, A. M. (2020). Transformers: State-of-the-Art Natural Language Processing [Conference paper]. 38–45. https://www.aclweb.org/anthology/2020.emnlp-demos.6\n",
    "- [Transformers](https://huggingface.co/docs/transformers/index)\n",
    "\n",
    "#### PyTorch\n",
    "Ansel, J., Yang, E., He, H., Gimelshein, N., Jain, A., Voznesensky, M., Bao, B., Bell, P., Berard, D., Burovski, E., Chauhan, G., Chourdia, A., Constable, W., Desmaison, A., DeVito, Z., Ellison, E., Feng, W., Gong, J., Gschwind, M., Hirsh, B., Huang, S., Kalambarkar, K., Kirsch, L., Lazos, M., Lezcano, M., Liang, Y., Liang, J., Lu, Y., Luk, C., Maher, B., Pan, Y., Puhrsch, C., Reso, M., Saroufim, M., Siraichi, M. Y., Suk, H., Suo, M., Tillet, P., Wang, E., Wang, X., Wen, W., Zhang, S., Zhao, X., Zhou, K., Zou, R., Mathews, A., Chanan, G., Wu, P., & Chintala, S. (2024). PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation [Conference paper]. 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24). https://doi.org/10.1145/3620665.3640366\n",
    "- [Start Locally | PyTorch](https://pytorch.org/get-started/locally)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Tools\n",
    "#### Optuna\n",
    "Akiba, T., Sano, S., Yanase, T., Ohta, T., & Koyama, M. (2019). Optuna: A next-generation hyperparameter optimization framework [Conference paper]. *Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining*, 2623–2631. https://doi.org/10.1145/3292500.3330701\n",
    "- [Optuna: A hyperparameter optimization framework](https://optuna.readthedocs.io/en/stable/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
