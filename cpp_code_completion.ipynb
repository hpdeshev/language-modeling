{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDM27gQHDWBl"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "import textwrap\n",
    "from typing import Literal\n",
    "\n",
    "from datasets import Dataset\n",
    "import optuna\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "  AutoConfig,\n",
    "  DataCollatorForLanguageModeling,\n",
    "  GPT2LMHeadModel, GPT2Tokenizer,\n",
    "  Trainer, TrainerCallback, TrainerControl,\n",
    "  TrainerState, TrainingArguments,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QL2UEUypcy2h"
   },
   "source": [
    "# Demonstration of GPT-2 architecture model pretraining for C++ code completion\n",
    "\n",
    "## Introduction\n",
    "In this notebook is demonstrated how a [GPT-2](#GPT-2) architecture model can be pretrained for C++ code completion, based on a single source file. The goal is to limit the required computing resources for training and evaluation. The demonstrated method utilizes the [Hugging Face Transformers](#Hugging-Face-Transformers) and [PyTorch](#PyTorch) deep learning libraries, and more specifically - the [DistilGPT2](#DistilGPT2) language model without pretrained weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVevv34wMVMV"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YO0_RJZgBYzn"
   },
   "outputs": [],
   "source": [
    "# General\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Data\n",
    "CPP_FILE = \"engine.cc\"\n",
    "DATA_DIR = \"data\"\n",
    "SAMPLE_SPAN = 3\n",
    "SAMPLE_STRIDE = 1\n",
    "\n",
    "# Optuna\n",
    "OPTUNA_ENABLED = False\n",
    "OPTUNA_EPOCHS = 2\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 4\n",
    "LEARN_RATE = 1e-4\n",
    "LEARN_RATE_SCHEDULER = \"inverse_sqrt\"\n",
    "MAX_TRAIN_EPOCHS = 1_000_000\n",
    "MODEL_NAME = \"distilbert/distilgpt2\"\n",
    "MODELS_DIR = \"models\"\n",
    "TRAIN_EPOCHS = 5\n",
    "VALIDATION_SPLIT = 0.2\n",
    "WARMUP_STEPS = 0\n",
    "WEIGHT_DECAY = 0.05\n",
    "\n",
    "# Text generation\n",
    "MIN_GEN_PROB = 0.95\n",
    "MAX_GEN_TOKENS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmLe5_F5gzvl"
   },
   "source": [
    "## Dataset retrieval\n",
    "\n",
    "For simplicity reasons, for dataset is selected a C++ source file `CPP_FILE` located at `DATA_DIR`. Hence, the dataset size depends on the size of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSN56LKWgzvl"
   },
   "outputs": [],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "raw_text = (Path() / DATA_DIR / CPP_FILE).read_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srqKC86FH1vH"
   },
   "source": [
    "## Tokenizer\n",
    "\n",
    "The [DistilGPT2](#DistilGPT2) model uses the [GPT-2](#GPT-2) tokenizer, which in turn uses [Byte pair encoding](#Byte-pair-encoding). Thus, spaces, new lines and braces, which are important not just for C++ but also for many other programming languages, are part of the vocabulary and their embeddings can be trained and fine tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Ky-3zAlH1vI"
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "inverted_vocab = {v: k for k, v in tokenizer.get_vocab().items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_muHj8_kl1zC"
   },
   "source": [
    "## Dataset preprocessing\n",
    "\n",
    "First, single-line comments, *#include*s and empty lines are removed.\n",
    "\n",
    "Then, based on the `SAMPLE_SPAN` and `SAMPLE_STRIDE` settings, the selected C++ source file is split into lines which are combined into potentially overlapping groups of length `SAMPLE_SPAN` and offset `SAMPLE_STRIDE`.\n",
    "\n",
    "The formed groups are then shuffled and split into training and validation sets, based on the `VALIDATION_SPLIT` setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MI42TOiCl5l4"
   },
   "outputs": [],
   "source": [
    "raw_lines = [re.sub(r\"([^\\s]*)\\s*//.*\", r\"\\1\", line)\n",
    "             for line in raw_text.splitlines() if line]\n",
    "raw_lines = [line\n",
    "             for line in raw_lines\n",
    "             if line and not line.lstrip().startswith(\"#include\")]\n",
    "\n",
    "samples = [\"\\n\".join(raw_lines[i:i+SAMPLE_SPAN])\n",
    "           for i in range(0, len(raw_lines)-SAMPLE_SPAN+1, SAMPLE_STRIDE)]\n",
    "random.shuffle(samples)\n",
    "\n",
    "val_idx = int(VALIDATION_SPLIT * len(samples))\n",
    "train_samples = samples[:-val_idx]\n",
    "train_ds = Dataset.from_dict(tokenizer(train_samples))\n",
    "val_samples = samples[-val_idx:]\n",
    "val_ds = Dataset.from_dict(tokenizer(val_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1YhnVSIr5fz"
   },
   "source": [
    "## Preparation for training\n",
    "\n",
    "Standard [Hugging Face Transformers](#Hugging-Face-Transformers) API for [PyTorch](#PyTorch), such as the `Trainer`, `TrainingArguments` and `TrainerCallback` classes, is utilized to configure the [DistilGPT2](#DistilGPT2) model for training.\n",
    "\n",
    "If the `OPTUNA_ENABLED` setting is *True*, then, instead of a training process, starts a hyperparameter tuning process via [Optuna](#Optuna). Thus, settings such as learning rate value and schedule, batch size and weight decay can be tuned.\n",
    "\n",
    "Additionally, a custom `CustomCallback` (based on `TrainerCallback`) is implemented to provide training control and, via the *evaluate_model* function, evaluation of the model's text generation capabilities on:\n",
    "- a single, randomly selected, C++ code sample per training epoch;\n",
    "- a custom C++ fragment during inference.\n",
    "\n",
    "The *evaluate_model* function displays various evaluation data such as:\n",
    "- the C++ code fragment;\n",
    "- the tokens' representation as per the tokenizer;\n",
    "- the prompt as per the *prompt_strategy* parameter, i.e., whether the first half of *cpp_text*, the second half or the whole *cpp_text* is to be used as prompt for code completion;\n",
    "- the generated code without the prompt, as per the *suggest_changes* function.\n",
    "\n",
    "The generated code is displayed only up to `MAX_GEN_TOKENS` and up to the part for which the model is confident, as per the `MIN_GEN_PROB` setting. Thus, during the initial training epochs there may be no generated code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FgeGzmUSr8BS"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "  model: GPT2LMHeadModel,\n",
    "  tokenizer: GPT2Tokenizer,\n",
    "  dataset: Dataset | None = None,\n",
    "  cpp_text: str | None = None,\n",
    "  prompt_strategy: Literal[\"start\", \"end\", \"all\"] = \"all\",\n",
    ") -> None:\n",
    "  if dataset is None and cpp_text is None:\n",
    "    raise ValueError(\"evaluate_model: \"\n",
    "                     \"one of dataset and cpp_text must be set.\")\n",
    "  model.eval()\n",
    "  if dataset is not None:\n",
    "    index = random.randint(0, len(dataset)-1)\n",
    "    ids = dataset[\"input_ids\"][index]\n",
    "    cpp_text = tokenizer.decode(ids)\n",
    "  else:\n",
    "    ids = tokenizer.encode(cpp_text)  # type: ignore\n",
    "  print(\"-\" * 80,\n",
    "        f\"[ C++ CODE ]\\n\\n{cpp_text}\",\n",
    "        sep=\"\\n\")\n",
    "\n",
    "  tokens_repr = \" \".join([inverted_vocab[i] for i in ids])\n",
    "  tokens_lines = textwrap.wrap(tokens_repr, width=80,\n",
    "                               expand_tabs=False,\n",
    "                               replace_whitespace=False,\n",
    "                               break_long_words=False,\n",
    "                               break_on_hyphens=False,\n",
    "                               drop_whitespace=False)\n",
    "  print(\"-\" * 80)\n",
    "  print(\"[ TOKENS ]\\n\\n\")\n",
    "  for line in tokens_lines:\n",
    "    print(line)\n",
    "\n",
    "  if prompt_strategy == \"start\":\n",
    "    prompt_ids = ids[:len(ids)//2]\n",
    "  elif prompt_strategy == \"end\":\n",
    "    prompt_ids = ids[len(ids)//2:]\n",
    "  else:\n",
    "    prompt_ids = ids\n",
    "  prompt = tokenizer.decode(prompt_ids)\n",
    "  print(\"-\" * 80,\n",
    "        f\"[ PROMPT ]\\n\\n{prompt}\",\n",
    "        sep=\"\\n\")\n",
    "\n",
    "  changes = suggest_changes(model, tokenizer,\n",
    "                            device_type=model.device.type,\n",
    "                            min_p=MIN_GEN_PROB, prompt=prompt)\n",
    "  print(\"-\" * 80,\n",
    "        f\"[ GENERATED ]\\n\\n{changes}\",\n",
    "        \"-\" * 80,\n",
    "        sep=\"\\n\")\n",
    "\n",
    "\n",
    "def suggest_changes(\n",
    "  model: GPT2LMHeadModel,\n",
    "  tokenizer: GPT2Tokenizer,\n",
    "  device_type: str,\n",
    "  prompt: str,\n",
    "  min_p: float = MIN_GEN_PROB,\n",
    ") -> str:\n",
    "  if not prompt:\n",
    "    raise ValueError(\"suggest_changes: prompt is empty.\")\n",
    "\n",
    "  ids: list[int] = tokenizer(prompt)[\"input_ids\"]  # type: ignore\n",
    "  input_len = len(ids)\n",
    "  while len(ids) - input_len < MAX_GEN_TOKENS:\n",
    "    logits = model(torch.tensor(ids).to(model.device)).logits\n",
    "    predictions = torch.softmax(logits[-1], dim=-1)\n",
    "    next_id = int(torch.argmax(predictions).item())\n",
    "    if predictions[next_id] < min_p:\n",
    "      break\n",
    "    ids += [next_id]\n",
    "  return tokenizer.decode(ids[input_len:])\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=MODELS_DIR,\n",
    "  eval_strategy=\"epoch\",\n",
    "  per_device_train_batch_size=BATCH_SIZE,\n",
    "  per_device_eval_batch_size=BATCH_SIZE,\n",
    "  learning_rate=LEARN_RATE,\n",
    "  lr_scheduler_type=LEARN_RATE_SCHEDULER,\n",
    "  warmup_steps=WARMUP_STEPS,\n",
    "  metric_for_best_model=\"eval_loss\",\n",
    "  save_strategy=\"no\" if OPTUNA_ENABLED else \"epoch\",\n",
    "  save_total_limit=1,\n",
    "  save_only_model=False,\n",
    "  seed=RANDOM_SEED,\n",
    "  dataloader_drop_last=True,\n",
    "  load_best_model_at_end=False,\n",
    "  weight_decay=WEIGHT_DECAY,\n",
    "  report_to=[\"none\"] if OPTUNA_ENABLED else [\"tensorboard\"],\n",
    "  push_to_hub=False,\n",
    "  num_train_epochs=OPTUNA_EPOCHS if OPTUNA_ENABLED else MAX_TRAIN_EPOCHS,\n",
    ")\n",
    "\n",
    "\n",
    "if not OPTUNA_ENABLED:\n",
    "  class CustomCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "      self._remaining_train_epochs = TRAIN_EPOCHS\n",
    "\n",
    "    def on_epoch_end(\n",
    "      self,\n",
    "      args: TrainingArguments,\n",
    "      state: TrainerState,\n",
    "      control: TrainerControl,\n",
    "      model: GPT2LMHeadModel,\n",
    "      processing_class: GPT2Tokenizer,\n",
    "      train_dataloader: DataLoader | None = None,\n",
    "      **kwargs,\n",
    "    ) -> None:\n",
    "      if train_dataloader is not None:\n",
    "        evaluate_model(model, processing_class,\n",
    "                       dataset=train_dataloader.dataset)  # type: ignore\n",
    "      self._remaining_train_epochs -= 1\n",
    "      if self._remaining_train_epochs == 0:\n",
    "        control.should_training_stop = True\n",
    "\n",
    "\n",
    "def model_init(trial: optuna.Trial) -> GPT2LMHeadModel:\n",
    "  try:\n",
    "    return GPT2LMHeadModel.from_pretrained(MODELS_DIR,\n",
    "                                           device_map=\"auto\",\n",
    "                                           torch_dtype=\"auto\")\n",
    "  except Exception as e:\n",
    "    print(\"model_init:\", e)\n",
    "    print(\"A model without pretrained weights to be loaded...\")\n",
    "    return GPT2LMHeadModel(AutoConfig.from_pretrained(MODEL_NAME))\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "  model_init=model_init,  # type: ignore\n",
    "  processing_class=tokenizer,\n",
    "  args=training_args,\n",
    "  train_dataset=train_ds,\n",
    "  eval_dataset=val_ds,\n",
    "  data_collator=DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    "  ),\n",
    "  callbacks=None if OPTUNA_ENABLED else [CustomCallback()],  # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBPbyMVLNy-r"
   },
   "source": [
    "## Training and evaluation\n",
    "\n",
    "If `OPTUNA_ENABLED` is *True*, then in the `optuna_hp_space` function are configured the trial parameters. The optimization process is performed with regard to the validation loss.\n",
    "\n",
    "If `OPTUNA_ENABLED` is *False*, then a training process starts. The final validation loss and perplexity metrics are reported once the training process completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KhH9LK6HmamW"
   },
   "outputs": [],
   "source": [
    "if OPTUNA_ENABLED:\n",
    "  def optuna_hp_space(trial: optuna.Trial) -> dict[str, str | float]:\n",
    "    return {\n",
    "      \"learning_rate\": trial.suggest_float(\n",
    "        \"learning_rate\", 1e-6, 1e-4, step=1e-6\n",
    "      ),\n",
    "      \"weight_decay\": trial.suggest_float(\n",
    "        \"weight_decay\", 0, 1e-1, step=1e-2\n",
    "      ),\n",
    "      \"lr_scheduler_type\": trial.suggest_categorical(\n",
    "        \"lr_scheduler_type\", [\"constant\", \"cosine\",\n",
    "                              \"inverse_sqrt\", \"linear\",\n",
    "                              \"reduce_lr_on_plateau\"]\n",
    "      ),\n",
    "      \"per_device_train_batch_size\": trial.suggest_categorical(\n",
    "        \"per_device_train_batch_size\", [4]\n",
    "      ),\n",
    "    }\n",
    "  optuna_sampler = optuna.samplers.TPESampler(seed=RANDOM_SEED)\n",
    "  best_trials = trainer.hyperparameter_search(\n",
    "    hp_space=optuna_hp_space,  # type: ignore\n",
    "    direction=\"minimize\",\n",
    "    backend=\"optuna\",\n",
    "    n_trials=20,\n",
    "    sampler=optuna_sampler,\n",
    "  )\n",
    "  print(best_trials)\n",
    "else:\n",
    "  last_checkpoint = get_last_checkpoint(MODELS_DIR)\n",
    "  trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "  eval_results = trainer.evaluate()\n",
    "  print(eval_results)\n",
    "  print(\"Perplexity: \"\n",
    "        f\"{torch.exp(torch.tensor(eval_results['eval_loss'])).item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdtyd5J4bUXD"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not OPTUNA_ENABLED:\n",
    "  evaluate_model(trainer.model, tokenizer,  # type: ignore\n",
    "                 cpp_text=samples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xA-LMCCWgzvp"
   },
   "source": [
    "## References\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### APA style for references\n",
    "American Psychological Association. (2022). Creating an APA Style reference list guide. https://apastyle.apa.org/instructional-aids/creating-reference-list.pdf\n",
    "\n",
    "American Psychological Association. (2024). APA Style common reference examples guide. https://apastyle.apa.org/instructional-aids/reference-examples.pdf\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Tokenization\n",
    "#### Byte-pair encoding\n",
    "Sennrich, R., Haddow, B., & Birch, A. (2015). Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909. https://arxiv.org/abs/1508.07909\n",
    "- [Byte pair encoding - Wikipedia](https://en.wikipedia.org/wiki/Byte_pair_encoding)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Machine learning models\n",
    "#### GPT-2\n",
    "Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI blog, 1(8), 9. https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
    "\n",
    "#### DistilGPT2\n",
    "Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108. https://arxiv.org/abs/1910.01108\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Guides and tutorials\n",
    "- [distilbert/distilgpt2 · Hugging Face](https://huggingface.co/distilbert/distilgpt2)\n",
    "- [Hugging Face - Documentation](https://huggingface.co/docs)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Libraries\n",
    "#### Hugging Face Transformers\n",
    "Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M., Lhoest, Q., & Rush, A. M. (2020). Transformers: State-of-the-Art Natural Language Processing [Conference paper]. 38–45. https://www.aclweb.org/anthology/2020.emnlp-demos.6\n",
    "- [Transformers](https://huggingface.co/docs/transformers/index)\n",
    "\n",
    "#### PyTorch\n",
    "Ansel, J., Yang, E., He, H., Gimelshein, N., Jain, A., Voznesensky, M., Bao, B., Bell, P., Berard, D., Burovski, E., Chauhan, G., Chourdia, A., Constable, W., Desmaison, A., DeVito, Z., Ellison, E., Feng, W., Gong, J., Gschwind, M., Hirsh, B., Huang, S., Kalambarkar, K., Kirsch, L., Lazos, M., Lezcano, M., Liang, Y., Liang, J., Lu, Y., Luk, C., Maher, B., Pan, Y., Puhrsch, C., Reso, M., Saroufim, M., Siraichi, M. Y., Suk, H., Suo, M., Tillet, P., Wang, E., Wang, X., Wen, W., Zhang, S., Zhao, X., Zhou, K., Zou, R., Mathews, A., Chanan, G., Wu, P., & Chintala, S. (2024). PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation [Conference paper]. 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24). https://doi.org/10.1145/3620665.3640366\n",
    "- [Start Locally | PyTorch](https://pytorch.org/get-started/locally)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Tools\n",
    "#### Optuna\n",
    "Akiba, T., Sano, S., Yanase, T., Ohta, T., & Koyama, M. (2019). Optuna: A next-generation hyperparameter optimization framework [Conference paper]. *Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining*, 2623–2631. https://doi.org/10.1145/3292500.3330701\n",
    "- [Optuna: A hyperparameter optimization framework](https://optuna.readthedocs.io/en/stable/)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
