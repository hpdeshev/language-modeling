{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDM27gQHDWBl"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import textwrap\n",
    "from typing import Literal\n",
    "\n",
    "from datasets import Dataset\n",
    "from langchain_classic.document_loaders import (\n",
    "  DirectoryLoader, TextLoader\n",
    ")\n",
    "from langchain_text_splitters import (\n",
    "  Language, RecursiveCharacterTextSplitter\n",
    ")\n",
    "import optuna\n",
    "from peft import (\n",
    "  get_peft_model, LoraConfig, PeftModelForCausalLM, TaskType\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "  AutoModelForCausalLM,\n",
    "  BitsAndBytesConfig,\n",
    "  DataCollatorForLanguageModeling,\n",
    "  EarlyStoppingCallback,\n",
    "  GPT2TokenizerFast,\n",
    "  TextGenerationPipeline,\n",
    "  Trainer, TrainerCallback, TrainerControl,\n",
    "  TrainerState, TrainingArguments,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QL2UEUypcy2h"
   },
   "source": [
    "# Demonstration of Granite-3B-Code-Base-2K model finetuning for C++ code completion\n",
    "\n",
    "## Introduction\n",
    "In this notebook is demonstrated how a [Granite-3B-Code-Base-2K](#Granite-3B-Code-Base-2K) model can be finetuned for C++ code completion, based on *.cc* and *.h* files in a folder. The goal is to limit the required computing resources through [quantization](https://huggingface.co/docs/transformers/quantization/overview) and [parameter-efficient fine-tuning (PEFT)](https://huggingface.co/docs/peft/index). The demonstrated method utilizes the [Hugging Face Transformers](#Hugging-Face-Transformers) and [PyTorch](#PyTorch) deep learning libraries, as well as the [LangChain](#LangChain) framework for creation of a dataset from C++ code chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVevv34wMVMV"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YO0_RJZgBYzn"
   },
   "outputs": [],
   "source": [
    "# General\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Data\n",
    "DATA_DIR = \"data\"\n",
    "CHUNK_SIZE = 240  # 3 * 80-char lines\n",
    "\n",
    "# Optuna\n",
    "OPTUNA_ENABLED = False\n",
    "OPTUNA_EPOCHS = 2\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 32\n",
    "EARLY_STOP_PATIENCE = 5\n",
    "LEARN_RATE = 1e-4\n",
    "LEARN_RATE_SCHEDULER = \"inverse_sqrt\"\n",
    "MAX_TRAIN_EPOCHS = 1_000_000\n",
    "MODEL_NAME = \"ibm-granite/granite-3b-code-base-2k\"\n",
    "MODELS_DIR = \"models\"\n",
    "TRAIN_EPOCHS = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "WARMUP_STEPS = 0\n",
    "WEIGHT_DECAY = 0.05\n",
    "\n",
    "# Text generation\n",
    "MAX_GEN_TOKENS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset retrieval\n",
    "\n",
    "The dataset constitutes `langchain_core.documents.Document` instances for all C++ source and header files found in `DATA_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "doc_loader = DirectoryLoader(DATA_DIR, glob=[\"**/*.cc\", \"**/*.h\"],\n",
    "                             loader_cls=TextLoader)\n",
    "docs = doc_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srqKC86FH1vH"
   },
   "source": [
    "## Tokenizer\n",
    "\n",
    "The [Granite-3B-Code-Base-2K](#Granite-3B-Code-Base-2K) model uses [Byte pair encoding](#Byte-pair-encoding) for its inputs. Thus, spaces, new lines and braces, which are important not just for C++ but also for many other programming languages, are part of the vocabulary and while their embeddings are \"freezed\", the model can still learn new and specific usages as per the codebase at `DATA_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Ky-3zAlH1vI"
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "inverted_vocab = {v: k for k, v in tokenizer.get_vocab().items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_muHj8_kl1zC"
   },
   "source": [
    "## Dataset preprocessing\n",
    "\n",
    "Based on `langchain_text_splitters.RecursiveCharacterTextSplitter` for `Language.CPP`, the `langchain_core.documents.Document` instances are combined into non-overlapping groups of maximal length `CHUNK_SIZE`.\n",
    "\n",
    "The formed groups are then shuffled and split into training and validation sets, based on the `VALIDATION_SPLIT` setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MI42TOiCl5l4"
   },
   "outputs": [],
   "source": [
    "cpp_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "  language=Language.CPP,\n",
    "  chunk_size=CHUNK_SIZE, chunk_overlap=0,\n",
    "  strip_whitespace=False,\n",
    ")\n",
    "cpp_chunks = cpp_splitter.split_documents(docs)\n",
    "random.shuffle(cpp_chunks)\n",
    "\n",
    "val_idx = int(VALIDATION_SPLIT * len(cpp_chunks))\n",
    "samples = [doc.page_content for doc in cpp_chunks]\n",
    "train_samples = samples[:-val_idx]\n",
    "train_ds = Dataset.from_dict(tokenizer(train_samples))\n",
    "val_samples = samples[-val_idx:]\n",
    "val_ds = Dataset.from_dict(tokenizer(val_samples))\n",
    "train_ds, val_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1YhnVSIr5fz"
   },
   "source": [
    "## Preparation for training\n",
    "\n",
    "In order to limit the system resources required for training, on the model is performed 8-bit [quantization](https://huggingface.co/docs/transformers/quantization/overview).\n",
    "In order to preserve the pretrained model weights, `PEFT` is performed, based on the model architecture.\n",
    "Standard [Hugging Face Transformers](#Hugging-Face-Transformers) API for [PyTorch](#PyTorch), such as the `Trainer`, `TrainingArguments` and `TrainerCallback` classes, is utilized to configure the [Granite-3B-Code-Base-2K](#Granite-3B-Code-Base-2K) model for training.\n",
    "\n",
    "If the `OPTUNA_ENABLED` setting is *True*, then, instead of a training process, starts a hyperparameter tuning process via [Optuna](#Optuna). Thus, settings such as learning rate value and schedule, batch size and weight decay can be tuned.\n",
    "\n",
    "Additionally, a custom `CustomCallback` (based on `TrainerCallback`) is implemented to provide training control and, via the *evaluate_model* function, evaluation of the model's text generation capabilities on:\n",
    "- a single, randomly selected, C++ code sample per training epoch;\n",
    "- a custom C++ fragment during inference.\n",
    "\n",
    "The *evaluate_model* function displays various evaluation data such as:\n",
    "- the C++ code fragment;\n",
    "- the tokens' representation as per the tokenizer;\n",
    "- the prompt as per the *prompt_strategy* parameter, i.e., whether the first half of *cpp_text*, the second half or the whole *cpp_text* is to be used as prompt for code completion;\n",
    "- the generated code without the prompt, as per the *suggest_changes* function.\n",
    "\n",
    "The generated code is displayed only up to `MAX_GEN_TOKENS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FgeGzmUSr8BS"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "  model: PeftModelForCausalLM,\n",
    "  tokenizer: GPT2TokenizerFast,\n",
    "  dataset: Dataset | None = None,\n",
    "  cpp_text: str | None = None,\n",
    "  prompt_strategy: Literal[\"start\", \"end\", \"all\"] = \"all\",\n",
    ") -> None:\n",
    "  if dataset is None and cpp_text is None:\n",
    "    raise ValueError(\"evaluate_model: \"\n",
    "                     \"one of 'dataset' and 'cpp_text' must be set.\")\n",
    "  model.eval()\n",
    "  if dataset is None:\n",
    "    ids = tokenizer.encode(cpp_text)\n",
    "  else:\n",
    "    index = random.randint(0, len(dataset)-1)\n",
    "    ids = dataset[\"input_ids\"][index]\n",
    "    cpp_text = tokenizer.decode(ids)\n",
    "  print(\"-\" * 80,\n",
    "        f\"[ C++ CODE ]\\n\\n{cpp_text}\",\n",
    "        sep=\"\\n\")\n",
    "\n",
    "  tokens_repr = \" \".join(inverted_vocab[i] for i in ids)\n",
    "  tokens_lines = textwrap.wrap(tokens_repr, width=80,\n",
    "                               expand_tabs=False,\n",
    "                               replace_whitespace=False,\n",
    "                               break_long_words=False,\n",
    "                               break_on_hyphens=False,\n",
    "                               drop_whitespace=False)\n",
    "  print(\"-\" * 80)\n",
    "  print(\"[ TOKENS ]\\n\\n\")\n",
    "  for line in tokens_lines:\n",
    "    print(line)\n",
    "\n",
    "  if prompt_strategy == \"start\":\n",
    "    prompt_ids = ids[:len(ids)//2]\n",
    "    prompt = tokenizer.decode(prompt_ids)\n",
    "  elif prompt_strategy == \"end\":\n",
    "    prompt_ids = ids[len(ids)//2:]\n",
    "    prompt = tokenizer.decode(prompt_ids)\n",
    "  else:\n",
    "    assert cpp_text is not None\n",
    "    prompt = cpp_text\n",
    "  print(\"-\" * 80,\n",
    "        f\"[ PROMPT ]\\n\\n{prompt}\",\n",
    "        sep=\"\\n\")\n",
    "\n",
    "  changes = suggest_changes(model, tokenizer, prompt=prompt)\n",
    "  print(\"-\" * 80,\n",
    "        f\"[ GENERATED ]\\n\\n{changes}\",\n",
    "        \"-\" * 80,\n",
    "        sep=\"\\n\")\n",
    "\n",
    "\n",
    "def suggest_changes(\n",
    "  model: PeftModelForCausalLM,\n",
    "  tokenizer: GPT2TokenizerFast,\n",
    "  prompt: str,\n",
    ") -> str:\n",
    "  if not prompt:\n",
    "    raise ValueError(\"suggest_changes: 'prompt' is empty.\")\n",
    "  pipe = TextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
    "  return pipe(prompt,\n",
    "              max_new_tokens=MAX_GEN_TOKENS,\n",
    "              return_full_text=False)[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=MODELS_DIR,\n",
    "  eval_strategy=\"epoch\",\n",
    "  per_device_train_batch_size=BATCH_SIZE,\n",
    "  per_device_eval_batch_size=BATCH_SIZE,\n",
    "  learning_rate=LEARN_RATE,\n",
    "  lr_scheduler_type=LEARN_RATE_SCHEDULER,\n",
    "  warmup_steps=WARMUP_STEPS,\n",
    "  metric_for_best_model=\"eval_loss\",\n",
    "  save_strategy=\"no\" if OPTUNA_ENABLED else \"epoch\",\n",
    "  save_total_limit=1,\n",
    "  save_only_model=False,\n",
    "  seed=RANDOM_SEED,\n",
    "  dataloader_drop_last=True,\n",
    "  load_best_model_at_end=not OPTUNA_ENABLED,\n",
    "  weight_decay=WEIGHT_DECAY,\n",
    "  report_to=[\"none\"] if OPTUNA_ENABLED else [\"tensorboard\"],\n",
    "  push_to_hub=False,\n",
    "  num_train_epochs=OPTUNA_EPOCHS if OPTUNA_ENABLED else MAX_TRAIN_EPOCHS,\n",
    ")\n",
    "\n",
    "\n",
    "if not OPTUNA_ENABLED:\n",
    "  class CustomCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "      self._remaining_train_epochs = TRAIN_EPOCHS\n",
    "\n",
    "    def on_epoch_end(  # type: ignore\n",
    "      self,\n",
    "      args: TrainingArguments,\n",
    "      state: TrainerState,\n",
    "      control: TrainerControl,\n",
    "      model: PeftModelForCausalLM,\n",
    "      processing_class: GPT2TokenizerFast,\n",
    "      train_dataloader: DataLoader | None = None,\n",
    "      **kwargs,\n",
    "    ) -> None:\n",
    "      if train_dataloader is not None:\n",
    "        evaluate_model(model, processing_class,\n",
    "                       dataset=train_dataloader.dataset)\n",
    "      self._remaining_train_epochs -= 1\n",
    "      if self._remaining_train_epochs == 0:\n",
    "        control.should_training_stop = True\n",
    "\n",
    "\n",
    "def model_init(trial: optuna.Trial) -> PeftModelForCausalLM:\n",
    "  quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "  model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    "  )\n",
    "  lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "  )\n",
    "  model = get_peft_model(model, lora_config)  # type: ignore\n",
    "  trainable_params = sum(p.numel()\n",
    "                         for p in model.parameters()\n",
    "                         if p.requires_grad)\n",
    "  print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "  return model  # type: ignore\n",
    "\n",
    "\n",
    "trainer_cbs = None\n",
    "if not OPTUNA_ENABLED:\n",
    "  trainer_cbs = [\n",
    "    CustomCallback(),\n",
    "    EarlyStoppingCallback(early_stopping_patience=EARLY_STOP_PATIENCE),\n",
    "  ]\n",
    "trainer = Trainer(\n",
    "  model_init=model_init,\n",
    "  processing_class=tokenizer,\n",
    "  args=training_args,\n",
    "  train_dataset=train_ds,\n",
    "  eval_dataset=val_ds,\n",
    "  data_collator=DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    "  ),\n",
    "  callbacks=trainer_cbs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBPbyMVLNy-r"
   },
   "source": [
    "## Training and evaluation\n",
    "\n",
    "If `OPTUNA_ENABLED` is *True*, then in the `optuna_hp_space` function are configured the trial parameters. The optimization process is performed with regard to the validation loss.\n",
    "\n",
    "If `OPTUNA_ENABLED` is *False*, then a training process starts. The final validation loss and perplexity metrics are reported once the training process completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KhH9LK6HmamW"
   },
   "outputs": [],
   "source": [
    "if OPTUNA_ENABLED:\n",
    "  def optuna_hp_space(trial: optuna.Trial) -> dict[str, str | float]:\n",
    "    return {\n",
    "      \"learning_rate\": trial.suggest_float(\n",
    "        \"learning_rate\", 1e-6, 1e-4, step=1e-6\n",
    "      ),\n",
    "      \"weight_decay\": trial.suggest_float(\n",
    "        \"weight_decay\", 0, 1e-1, step=1e-2\n",
    "      ),\n",
    "      \"lr_scheduler_type\": trial.suggest_categorical(\n",
    "        \"lr_scheduler_type\", [\"constant\", \"cosine\",\n",
    "                              \"inverse_sqrt\", \"linear\",\n",
    "                              \"reduce_lr_on_plateau\"]\n",
    "      ),\n",
    "      \"per_device_train_batch_size\": trial.suggest_categorical(\n",
    "        \"per_device_train_batch_size\", [32]\n",
    "      ),\n",
    "    }\n",
    "  optuna_sampler = optuna.samplers.TPESampler(seed=RANDOM_SEED)\n",
    "  best_trials = trainer.hyperparameter_search(\n",
    "    hp_space=optuna_hp_space,\n",
    "    direction=\"minimize\",\n",
    "    backend=\"optuna\",\n",
    "    n_trials=20,\n",
    "    sampler=optuna_sampler,\n",
    "  )\n",
    "  print(best_trials)\n",
    "else:\n",
    "  last_checkpoint = get_last_checkpoint(MODELS_DIR)\n",
    "  trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "  eval_results = trainer.evaluate()\n",
    "  print(eval_results)\n",
    "  print(\"Perplexity: \"\n",
    "        f\"{torch.exp(torch.tensor(eval_results['eval_loss'])).item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdtyd5J4bUXD"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFONsS-tUR95"
   },
   "outputs": [],
   "source": [
    "if not OPTUNA_ENABLED:\n",
    "  evaluate_model(trainer.model, tokenizer,\n",
    "                 cpp_text=val_samples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xA-LMCCWgzvp"
   },
   "source": [
    "## References\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### APA style for references\n",
    "American Psychological Association. (2022). Creating an APA Style reference list guide. https://apastyle.apa.org/instructional-aids/creating-reference-list.pdf\n",
    "\n",
    "American Psychological Association. (2024). APA Style common reference examples guide. https://apastyle.apa.org/instructional-aids/reference-examples.pdf\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Tokenization\n",
    "<a name=\"Byte-pair-encoding\"></a>\n",
    "#### Byte-pair encoding\n",
    "Sennrich, R., Haddow, B., & Birch, A. (2015). Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909. https://arxiv.org/abs/1508.07909\n",
    "- [Byte pair encoding - Wikipedia](https://en.wikipedia.org/wiki/Byte_pair_encoding)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Machine learning models\n",
    "<a name=\"Granite-3B-Code-Base-2K\"></a>\n",
    "#### Granite-3B-Code-Base-2K\n",
    "Mishra, M., Stallone, M., Zhang, G., Shen, Y., Prasad, A., Soria, A.M., Merler, M., Selvam, P., Surendran, S., Singh, S., Sethi, M., Dang, X., Li, P., Wu, K., Zawad, S., Coleman, A., White, M., Lewis, M., Pavuluri, R., Koyfman, Y., Lublinsky, B., Bayser, M.D., Abdelaziz, I., Basu, K., Agarwal, M., Zhou, Y., Johnson, C., Goyal, A., Patel, H., Shah, Y., Zerfos, P., Ludwig, H., Munawar, A., Crouse, M., Kapanipathi, P., Salaria, S., Calio, B., Wen, S., Seelam, S.R., Belgodere, B.M., Fonseca, C., Singhee, A., Desai, N., Cox, D.D., Puri, R., & Panda, R. (2024). Granite Code Models: A Family of Open Foundation Models for Code Intelligence. ArXiv, abs/2405.04324. https://arxiv.org/abs/2405.04324\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Guides and tutorials\n",
    "- [Introduction | ü¶úÔ∏èüîó LangChain](https://python.langchain.com/docs/introduction/)\n",
    "- [ibm-granite/granite-3b-code-base-2k ¬∑ Hugging Face](https://huggingface.co/ibm-granite/granite-3b-code-base-2k)\n",
    "- [Hugging Face - Documentation](https://huggingface.co/docs)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Libraries and frameworks\n",
    "<a name=\"Hugging-Face-Transformers\"></a>\n",
    "#### Hugging Face Transformers\n",
    "Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M., Lhoest, Q., & Rush, A. M. (2020). Transformers: State-of-the-Art Natural Language Processing [Conference paper]. 38‚Äì45. https://www.aclweb.org/anthology/2020.emnlp-demos.6\n",
    "- [Transformers](https://huggingface.co/docs/transformers/index)\n",
    "\n",
    "<a name=\"LangChain\"></a>\n",
    "#### LangChain\n",
    "Chase, H. (2022). LangChain [Computer software]. https://github.com/langchain-ai/langchain\n",
    "- [Introduction | ü¶úÔ∏èüîó LangChain](https://python.langchain.com/docs/introduction/)\n",
    "\n",
    "<a name=\"PyTorch\"></a>\n",
    "#### PyTorch\n",
    "Ansel, J., Yang, E., He, H., Gimelshein, N., Jain, A., Voznesensky, M., Bao, B., Bell, P., Berard, D., Burovski, E., Chauhan, G., Chourdia, A., Constable, W., Desmaison, A., DeVito, Z., Ellison, E., Feng, W., Gong, J., Gschwind, M., Hirsh, B., Huang, S., Kalambarkar, K., Kirsch, L., Lazos, M., Lezcano, M., Liang, Y., Liang, J., Lu, Y., Luk, C., Maher, B., Pan, Y., Puhrsch, C., Reso, M., Saroufim, M., Siraichi, M. Y., Suk, H., Suo, M., Tillet, P., Wang, E., Wang, X., Wen, W., Zhang, S., Zhao, X., Zhou, K., Zou, R., Mathews, A., Chanan, G., Wu, P., & Chintala, S. (2024). PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation [Conference paper]. 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24). https://doi.org/10.1145/3620665.3640366\n",
    "- [Start Locally | PyTorch](https://pytorch.org/get-started/locally)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Tools\n",
    "<a name=\"Optuna\"></a>\n",
    "#### Optuna\n",
    "Akiba, T., Sano, S., Yanase, T., Ohta, T., & Koyama, M. (2019). Optuna: A next-generation hyperparameter optimization framework [Conference paper]. *Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining*, 2623‚Äì2631. https://doi.org/10.1145/3292500.3330701\n",
    "- [Optuna: A hyperparameter optimization framework](https://optuna.readthedocs.io/en/stable/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
